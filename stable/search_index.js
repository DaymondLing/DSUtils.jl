var documenterSearchIndex = {"docs":
[{"location":"Reference.html","page":"Function Reference","title":"Function Reference","text":"CurrentModule = DSUtils","category":"page"},{"location":"Reference.html","page":"Function Reference","title":"Function Reference","text":"","category":"page"},{"location":"Reference.html","page":"Function Reference","title":"Function Reference","text":"Modules = [DSUtils]","category":"page"},{"location":"Reference.html#DSUtils.BCDiag","page":"Function Reference","title":"DSUtils.BCDiag","text":"BCDiag\n\nA structure of diagnostic properties of a binary Classifier. Facilitates summary plots and tables.\n\n\n\n\n\n","category":"type"},{"location":"Reference.html#DSUtils.accuracyplot-Tuple{BCDiag}","page":"Function Reference","title":"DSUtils.accuracyplot","text":"accuracyplot(x::BCDiag; util=[1, 0, 0, 1])\n\nUsing util values for [TP, FN, FP, TN], produce accuracy plot and its [max, argmax, argdep]. Default util values of [1, 0, 0, 1] gives the standard accuracy value of (TP+TN)/N.\n\n\n\n\n\n","category":"method"},{"location":"Reference.html#DSUtils.auroc-Tuple{BitArray{1},Array{T,1} where T}","page":"Function Reference","title":"DSUtils.auroc","text":"auroc(class, var; tie = 1e-6)\n\nCalculate area under Receiver Operating Characteristics (ROC) curve, class is a 2 level categorical variable, var is the distribution to analyze. Pair-wise comparison between class 1 values with class 0 values are made as follows:\n\nclass 1 value > class 0 value is Concordant\nclass 1 value ≈ class 0 value (within tie) is Tied\nclass 1 value < class 0 value is Discordant\n\nReturns:\n\nconcordant, number of concordant comparisons\ntied, number of tied comparisons\ndiscordant, number of discordant comparisons\nauc, or C, is (Concordant + 0.5Tied) / Total comparisons; same as numeric integration of ROC curve\ngini, 2C-1, also known as Somer's D, is (Concordant - Discordant) / Total comparisons\n\nNote there are other rank order measures:\n\nGoodman-Kruskal Gamma is (Concordant - Discordant) / (Concordant + Discordant), no penalty for ties\nKendall's Tau is (Concordant - Discordant) / (0.5 N(N-1))\n\n\n\n\n\n","category":"method"},{"location":"Reference.html#DSUtils.bcdiag-Tuple{BitArray{1},Array{T,1} where T}","page":"Function Reference","title":"DSUtils.bcdiag","text":"bcdiag(target, pred; groups = 100, rev = true, tie = 1e-6)\n\nPerform diagnostics of a binary classifier. target is a 2 level categorical variable, pred is probability of class 1. groups is the number of bins to use for plotting/printing. rev = true orders pred from high to low. tie is the tolerance of pred where values are considered tied.\n\nReturns a BCDiag struct which can be used for plotting or printing:\n\nbiasplot is calibration plot of target response rate vs. pred response rate\nksplot produces ksplot of cumulative distributions\nrocplot plots the Receiver Operating Characteristics curve\naccuracyplot plots the accuracy curve with adjustable utility\nliftcurve is the lift curve\ncumliftsurve is the cumulative lift surve\nliftable is the lift table as a DataFrame\ncumliftable is the cumulative lift table as a DataFrame\n\n\n\n\n\n","category":"method"},{"location":"Reference.html#DSUtils.biasplot-Tuple{BCDiag}","page":"Function Reference","title":"DSUtils.biasplot","text":"biasplot(x::BCDiag)\n\nreturns a bias calibration plot of x - actual response vs. predicted response\n\n\n\n\n\n","category":"method"},{"location":"Reference.html#DSUtils.concordance-Tuple{BitArray{1},Array{T,1} where T,Any}","page":"Function Reference","title":"DSUtils.concordance","text":"concordance(class, var, tie)\n\nConcordance calculation with flexible tied region (auroc uses fixed width region). class is a 2 level categorical variable, var is the distribution to analyze, tie(x) returns the lower and upper bound of tied region of x.\n\nPair-wise comparison between class 1 values with class 0 values are made as follows: class 1 value > class 0 value is Concordant; class 1 value ≈ class 0 value (within tie) is Tied; class 1 value < class 0 value is Discordant.\n\nReturns:\n\nconcordant, number of concordant comparisons\ntied, number of tied comparisons\ndiscordant, number of discordant comparisons\nauroc, or C, is (Concordant + 0.5Tied) / Total comparisons; same as numeric integration of ROC curve\ngini, 2C-1, also known as Somer's D, is (Concordant - Discordant) / Total comparisons\n\nNote Goodman-Kruskal Gamma is (Concordant - Discordant) / (Concordant + Discordant); and Kendall's Tau is (Concordant - Discordant) / (0.5 x Total count x (Total count - 1))\n\n\n\n\n\n","category":"method"},{"location":"Reference.html#DSUtils.cumliftable-Tuple{BCDiag}","page":"Function Reference","title":"DSUtils.cumliftable","text":"cumliftable(x::BCDiag)\n\nreturns a cumulative lift table of x as a DataFrame\n\n\n\n\n\n","category":"method"},{"location":"Reference.html#DSUtils.cumliftcurve-Tuple{BCDiag}","page":"Function Reference","title":"DSUtils.cumliftcurve","text":"cumliftcurve(x::BCDiag)\n\nreturns a cumulative lift curve plot of x - cumulative actual and predicted vs. depth\n\n\n\n\n\n","category":"method"},{"location":"Reference.html#DSUtils.infovalue-Tuple{Array{TG,1} where TG<:Integer,Array{TB,1} where TB<:Integer}","page":"Function Reference","title":"DSUtils.infovalue","text":"infovalue(g::Vector{Integer}, b::Vector{Integer}))\n\nInformation value calculation of g, b vector of binned frequency counts\n\nweight of evidence = log(density g / density b), 0 adjusted\ninfovalue = sum (density g - density b) * weight of evidence\n\nIndustry rule of thumb:\n\niv <= 0.1         no significant change\n0.1 < iv <= 0.25  minor change\n0.25 < iv         major change\n\n\n\n\n\n","category":"method"},{"location":"Reference.html#DSUtils.ksplot-Tuple{BCDiag}","page":"Function Reference","title":"DSUtils.ksplot","text":"ksplot(x::BCDiag)\n\nreturns a KS plot of x - CDF1 (True Positive) and CDF0 (False Positive) versus depth\n\n\n\n\n\n","category":"method"},{"location":"Reference.html#DSUtils.kstest-Tuple{BitArray{1},Array{T,1} where T}","page":"Function Reference","title":"DSUtils.kstest","text":"kstest(class, var; rev = true)\n\nCalculate empirical 2 sample Kolmogorov-Smirnov statistic and its location. class is a 2 level categorical variable, var is the distribution to analyze.\n\nReturns:\n\nn, total number of observations\nn1, number of observations of class 1\nn0, number of observations of class 0\nbaserate, incidence rate of class 1\nks, the maximum separation between the two cumulative distributions\nksarg, the value of var at which maximum separation is achieved\nksdep, depth of ksarg in the sorted values of var;\n\nrev = true counts depth from high value towards low value.\n\n\n\n\n\n","category":"method"},{"location":"Reference.html#DSUtils.liftable-Tuple{BCDiag}","page":"Function Reference","title":"DSUtils.liftable","text":"liftable(x::BCDiag)\n\nreturns a lift table of x as a DataFrame\n\n\n\n\n\n","category":"method"},{"location":"Reference.html#DSUtils.liftcurve-Tuple{BCDiag}","page":"Function Reference","title":"DSUtils.liftcurve","text":"liftcurve(x::BCDiag)\n\nreturns a lift curve plot of x - actual and predicted versus depth\n\n\n\n\n\n","category":"method"},{"location":"Reference.html#DSUtils.onehot!-Tuple{AbstractDataFrame,Array{Symbol,1}}","page":"Function Reference","title":"DSUtils.onehot!","text":"onehot!(df::AbstractDataFrame, vars::Vector{Symbol})\n\nOne hot encode variables in vars in df.\n\n\n\n\n\n","category":"method"},{"location":"Reference.html#DSUtils.onehot!-Tuple{AbstractDataFrame,Symbol}","page":"Function Reference","title":"DSUtils.onehot!","text":"onehot!(df::AbstractDataFrame, var::Symbol)\n\nOne hot encode unique values of var in df. New variable name is constructed as var_lvl. If df.var is string, it is standardized first via strstd before one hot encoding.\n\n\n\n\n\n","category":"method"},{"location":"Reference.html#DSUtils.onehot-Union{Tuple{T}, Tuple{Missing,Array{T,1}}} where T>:Missing","page":"Function Reference","title":"DSUtils.onehot","text":"onehot(lvl, x::Vector)\n\nCreate an indicator variable of x equal to lvl. lvl and x can have missing values, ismissing is used where necessary to ensure correct result.\n\n\n\n\n\n","category":"method"},{"location":"Reference.html#DSUtils.ranks-Tuple{Array{T,1} where T<:Real}","page":"Function Reference","title":"DSUtils.ranks","text":"ranks(x; groups = 10, rank = tiedrank, rev = false)\n\nReturn a variable which bins x into groups number of bins. The rank keyword allows different ranking method; use rev = true to reverse sort so that small bin number is large value of x. Missing values are assigned to group missing.\n\nDefault values of rank = tiedrank and rev = false results in similar grouping as SAS PROC RANK groups=n tied=mean.\n\n\n\n\n\n","category":"method"},{"location":"Reference.html#DSUtils.rocplot-Tuple{BCDiag}","page":"Function Reference","title":"DSUtils.rocplot","text":"rocplot(x::BCDiag)\n\nreturns a ROC plot of x - CDF1 (True Positive) vs. CDF0 (False Positive)\n\n\n\n\n\n","category":"method"},{"location":"Reference.html#DSUtils.strstd-Tuple{AbstractString}","page":"Function Reference","title":"DSUtils.strstd","text":"strstd(s; empty = \"\")\n\nStandardizes string s by stripping leading and trailing blanks, embedded blanks are replaced with underscore _. If s is missing or all blanks, the result is value of empty.\n\nDefault of empty is \"\", missing values and all blank strings become \"\".\n\n\n\n\n\n","category":"method"},{"location":"Reference.html#DSUtils.sumxm-Tuple","page":"Function Reference","title":"DSUtils.sumxm","text":"sumxm(x...)     sum treating missings as 0\n\nReturns the sum of x treating missing values as 0's. x can be varying number of scalars, in this case, their sum is returned. If x is varying number of vectors, they are summed element-wise across the vectors.\n\n\n\n\n\n","category":"method"},{"location":"man/bcdiag.html#Binary-classifier-performance-evaluation","page":"Binary Classifier","title":"Binary classifier performance evaluation","text":"","category":"section"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"Whether you are building a binary classifier or need to audit one built by someone else, there are many things we'd like to know about its performance. The following sections describe functions that are designed to let you easily get at commonly used binary classifier performance diagnostic metrics.","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"The functions are relatively performant and are capable of handling millions of rows of data.","category":"page"},{"location":"man/bcdiag.html#kstest","page":"Binary Classifier","title":"kstest","text":"","category":"section"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"The two sample Kolmogorov-Smirnov test is a statistical test of whether two empirical distributions are the same. The test is based on finding the maximum separation between the two cumulative distribution functions (CDF) and determining the p-value of the test statistic.","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"For binary classifiers, the predicted probabilities of the two classes should be different, thus the interest isn't whether the probability distributions are different, rather, it is how large is the maximal separation and where does it occur.","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"Let's generate some data to illustrate the idea.","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"using Random, Distributions, Plots\r\n\r\nRandom.seed!(123)\r\n\r\nn100 = rand(Normal(100, 10), 1000)\r\nn100a = rand(Normal(100, 10), 1000)\r\nn120 = rand(Normal(120, 10), 1000)\r\nn140 = rand(Normal(140, 10), 1000)\r\n\r\nhistogram(n100, nbins = 50, opacity= 0.3)\r\nhistogram!(n100a, nbins = 50, opacity= 0.3, legend = nothing)\r\nsavefig(\"kstest-1.svg\"); nothing # hide","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"(Image: )","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"We can use the kstest function to find the maximum separation and its location. The required input is a vector designating the two classes and another vector of the values, this is the typical data structure of model scoring on development or validation data.","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"using DSUtils\r\n\r\ncls = [fill(0, length(n100)); fill(1, length(n100a))]\r\nvalues = [n100; n100a]\r\nkstest(cls, values)","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"kstest returns results in a named tuple:","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"n, total number of observations\nn1, total number of observations in class 1\nn0, total number of observations in class 0\nbaserate, n1 / n, the incidence rate of class 1\nks, the maximum separation between CDF1 and CDF0, a value between [0, 1]\nksarg, argmax, the value where maximum separation is achieved\nksdep, depth of argmax in the sorted values (default sort is from high to low)","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"ks of 0 means the distributions are indistinguishable, ks of 1 says the two distributions are complete separable. These two distributions have negligible separation since they are drawn from the same distribution.","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"We now test on moderate separation:","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"histogram(n100, nbins = 50, opacity= 0.3)\r\nhistogram!(n120, nbins = 50, opacity= 0.3, legend = nothing)\r\nsavefig(\"kstest-2.svg\"); nothing    # hide","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"(Image: )","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"cls = [fill(0, length(n100)); fill(1, length(n120))]\r\nvalues = [n100; n120]\r\nkstest(cls, values)","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"There's considerable separation between the classes, and ks is larger than before.","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"Let's test on widely separately data:","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"histogram(n100, nbins = 50, opacity= 0.3)\r\nhistogram!(n140, nbins = 50, opacity= 0.3, legend = nothing)\r\nsavefig(\"kstest-3.svg\"); nothing    # hide","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"(Image: )","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"cls = [fill(0, length(n100)); fill(1, length(n140))]\r\nvalues = [n100; n140]\r\nkstest(cls, values)","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"We can see that the two classes are nearly separable and ks is now quite high at 0.949. These examples illustrate how ks can serve as an indicator of the ability to separate the two classes.","category":"page"},{"location":"man/bcdiag.html#auroc","page":"Binary Classifier","title":"auroc","text":"","category":"section"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"A good binary classifier would have high sensitivity (able to recognize True Positive) and high specificity (able to recognize True Negatives, hence have low False Positive). A plot of the trade-off curve of True Positive Rate versus False Positive Rate at various cutoff probabilities is called the Receiver Operating Characteristics (ROC) curve. One way to quantify performance is by the area under the ROC curve, often abbreviated as AUC or C, many packages would compute AUC via numeric integration of the ROC curve. AUC is in the range [0, 1], a perfect model has AUC of 1, a random model has AUC of 0.5, and a perfectly backwards model would have AUC of -1.","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"There is another interpretation of AUC which provides more intuition than simply as the area under a curve. If we make all possible pair-wise comparisons between the probabilities of class 1 with class 0, we can count the incidences of:","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"Concordant: class 1 probability > class 0 probability\nTied: class 1 probability ≈ class 0 probability\nDiscordant: class 1 probability < class 0 probability","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"Then we can compute:","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"AUC: (Concordant + 0.5 Tied) / (N1 * N0)\nGini: 2AUC - 1, or (Concordant - Discordant) / (N1 * N0)\nGoodman-Kruskal Gamma: (Concordant - Discordant) / (Concordant + Discordant),","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"no penalty for Tied","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"Kendall's Tau: (Concordant - Discordant) / (0.5 * (N1+N0) * (N1+N0-1))","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"We can interpret AUC as the percentage of time class 1 probabilities is larger than class 0 probabilities (ignoring ties).","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"The mathematical proof can be found at Stack Exchange and Professor David J. Hand's article.","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"cls = [fill(0, length(n100)); fill(1, length(n140))]\r\nvalues = [n100; n140]\r\nauroc(cls, values)","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"auroc returns results in a named tuple:","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"conc, number of concordant comparisons\ntied, number of tied comparisons\ndisc, number of discordant comparisons\nauc, area under ROC curve, or just area under curve\ngini, 2auc - 1","category":"page"},{"location":"man/bcdiag.html#bcdiag","page":"Binary Classifier","title":"bcdiag","text":"","category":"section"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"While kstest and auroc provide diagnostic measures for comparing model performance, when there is a model of interest, it is likely that we need to produce many graphs and table to understand and document its performance, bcdiag allows us to do this easily.","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"using Random\r\nusing GLM\r\nusing DSUtils\r\n\r\nfunction logitgen(intercept::Real, slope::Real, len::Int; seed = 888)\r\n    Random.seed!(seed)\r\n    x = 10 .* rand(len)                     # random uniform [0, 10)\r\n    # sort!(x)                                # x ascending\r\n    logit = @. intercept + slope * x        # logit(prob) = ln(p / (1 + p)) = linear equation\r\n    prob = @. 1. / (1. + exp(-logit))       # probability\r\n    y = rand(len) .<= prob\r\n    y, x\r\nend\r\n\r\nm = DataFrame(logitgen(-3, 0.6, 100_000), (:target, :x))\r\nm_logistic = glm(@formula(target ~ x), m, Binomial(), LogitLink())\r\nm.pred = predict(m_logistic)\r\n\r\nkstest(m.target, m.pred)","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"auroc(m.target, m.pred)","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"Running bcdiag prints a quick summary:","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"mdiag = bcdiag(m.target, m.pred)","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"The output structure allows us to create the following plots and tables to understand:","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"the ability of the model to separate the two classes\nthe accuracy of the probability point estimates\nhow to set cutoff for maximum accuracy\nperformance of the model at varying cutoff depth","category":"page"},{"location":"man/bcdiag.html#ksplot","page":"Binary Classifier","title":"ksplot","text":"","category":"section"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"ksplot plots the cumulative distribution of class 1 (true positive rate) and class 0 (false positive rate) versus depth.","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"ksplot(mdiag)\r\nsavefig(\"bcd-ksplot.svg\"); nothing # hide","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"It shows where the maximum separation of the two distributions occur.","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"(Image: )","category":"page"},{"location":"man/bcdiag.html#rocplot","page":"Binary Classifier","title":"rocplot","text":"","category":"section"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"rocplot plots the true positive rate vs. false positive rate (depth is implicit).","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"rocplot(mdiag)\r\nsavefig(\"bcd-rocplot.svg\"); nothing # hide","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"A perfect model has auc of 1, a random model has auc of 0.5.","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"(Image: )","category":"page"},{"location":"man/bcdiag.html#biasplot","page":"Binary Classifier","title":"biasplot","text":"","category":"section"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"Both ksplot and rocplot rely on the ability of the model to rank order the observations, the score value itself doesn't matter. For example, if you took the score and perform any monotonic transform, ks and auc wouldn't change. There are occasions where the score value does matter, where the probabilities need to be accurate, for example, in expected return calculations. Thus, we need to understand whether the probabilities are accurate, biasplot does this by plotting the observed response rate versus predicted response rate to look for systemic bias. This is also called the calibration graph.","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"biasplot(mdiag)\r\nsavefig(\"bcd-biasplot.svg\"); nothing # hide","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"(Image: )","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"An unbiased model would lie on the diagnonal, systemic shift off the diagonal represents over or under estimate of the true probability.","category":"page"},{"location":"man/bcdiag.html#accuracyplot","page":"Binary Classifier","title":"accuracyplot","text":"","category":"section"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"People often refer to (TP + TN) / N as accuracy of the model, that is, the ability to correctly identify correct cases. It is used to compare model performance as well - model with higher accuracy is a better model. For a probability based classifier, a cutoff is required to turn probability to predicted class. So, what is the cutoff value to use to achieve maximum accuracy?","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"There are many approaches to setting the best cutoff, one way is to assign utility values to the four outcomes of [TP, FP, FN, TN] and maximize the sum across different cutoff's. Accuracy measure uses the utility values of [1, 0, 0, 1] giving TP + TN. You can assign negative penalty terms for misclassification as well.","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"Note that this is different from kstest - maximum separation on cumulative distribution (normalized to 100%) does not account for class size difference, e.g., class 1 may be only 2% of the cases.","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"accuracyplot(mdiag)\r\nsavefig(\"bcd-accuracyplot.svg\"); nothing # hide","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"(Image: )","category":"page"},{"location":"man/bcdiag.html#liftcurve","page":"Binary Classifier","title":"liftcurve","text":"","category":"section"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"liftcurve plots the actual response and predicted response versus depth, with baserate as 1.","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"liftcurve(mdiag)\r\nsavefig(\"bcd-liftcurve.svg\"); nothing # hide","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"We can easily see where the model is performing better than average, approximately the same as average, or below average.","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"(Image: )","category":"page"},{"location":"man/bcdiag.html#cumliftcurve","page":"Binary Classifier","title":"cumliftcurve","text":"","category":"section"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"cumliftcurve is similar to liftcurve, the difference is it is a plot of cumulative response rate from the top of the model.","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"cumliftcurve(mdiag)\r\nsavefig(\"bcd-cumliftcurve.svg\"); nothing # hide","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"(Image: )","category":"page"},{"location":"man/bcdiag.html#liftable","page":"Binary Classifier","title":"liftable","text":"","category":"section"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"liftable is the table from which liftcurve is plotted.","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"liftable(mdiag)","category":"page"},{"location":"man/bcdiag.html#cumliftable","page":"Binary Classifier","title":"cumliftable","text":"","category":"section"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"cumliftable is the cumulative version of liftable.","category":"page"},{"location":"man/bcdiag.html","page":"Binary Classifier","title":"Binary Classifier","text":"cumliftable(mdiag)","category":"page"},{"location":"man/categorical.html#Categorical","page":"Categorical encoding","title":"Categorical","text":"","category":"section"},{"location":"man/categorical.html","page":"Categorical encoding","title":"Categorical encoding","text":"Reference encoding\nOne hot encoding\nHelmert encoding","category":"page"},{"location":"man/dataprep.html#Data-Pre-processing","page":"Data preparation","title":"Data Pre-processing","text":"","category":"section"},{"location":"man/dataprep.html","page":"Data preparation","title":"Data preparation","text":"Data wrangling","category":"page"},{"location":"man/dataprep.html#String-standardization","page":"Data preparation","title":"String standardization","text":"","category":"section"},{"location":"man/dataprep.html","page":"Data preparation","title":"Data preparation","text":"string with blanks and missings","category":"page"},{"location":"man/dataprep.html#Arithmetic-with-missing","page":"Data preparation","title":"Arithmetic with missing","text":"","category":"section"},{"location":"man/dataprep.html","page":"Data preparation","title":"Data preparation","text":"missing","category":"page"},{"location":"man/dataprep.html#One-hot-encoding","page":"Data preparation","title":"One hot encoding","text":"","category":"section"},{"location":"man/dataprep.html","page":"Data preparation","title":"Data preparation","text":"11111","category":"page"},{"location":"man/dataprep.html","page":"Data preparation","title":"Data preparation","text":"works for missing values","category":"page"},{"location":"index.html#DSUtils.jl","page":"Home","title":"DSUtils.jl","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"A set of Data Science utilities in Julia for industry practitioners.","category":"page"},{"location":"index.html#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"This package is not registered in Julia's general registry. You can add it via its URL:","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"] add https://github.com/DaymondLing/DSUtils.jl","category":"page"},{"location":"index.html#Purpose","page":"Home","title":"Purpose","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"The intent of DSUtils is to make it easy for industry practitioners to get their work done. It provides functions for some routine parts of data science workflow so that people can focus on problem solving and not break their train of thought. The functions should just work and be reasonably performant on millions of rows of data.","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"There are no new capabilities per se, indeed, many of the functionalities are available in other packages. It is an end user package to get analysis done rather than a package for others to build packages with.","category":"page"},{"location":"index.html#Current-capabilities","page":"Home","title":"Current capabilities","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"Binary classifier performance evaluation\nkstest, 2 sample Kolmogorov-Smirnov point estimate and location\nauroc, Area Under Receiver Operatin Characteristics curve via   concordance calculation rather than numeric integration\nbcdiag, wrapper for kstest and auroc that facilitates   plotting the graphs below\nksplot, plot of Kolmogorov-Smirnov separation\nrocplot, ROC plot\nbiasplot, plot of actual response rate vs. predicted probability\naccuracyplot, plot of accuracy given utility values for [TP, FN, FP, TN]\nliftcurve, actual and predicted lift curves\ncumliftcurve, cumulative actual and predicted lift curves\nliftable, actual and predicted lift tables\ncumliftable, cumulative actual and predicted lift tables\ninfovalue, change in two frequency distributions","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"Binning\nranks, equal density binning, tied values are always in the same bin","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"Dummy encoding\nonehot!, one hot encode a categorical variable, works with missing level   and missing data, output indicator variables are in a DataFrame   with named columns","category":"page"},{"location":"index.html","page":"Home","title":"Home","text":"Missing computation\nsumxm, sum a collection of scalars or vectors (element-wise) treating missing as 0","category":"page"},{"location":"index.html#Project-Status","page":"Home","title":"Project Status","text":"","category":"section"},{"location":"index.html","page":"Home","title":"Home","text":"This is work in progress, development is against the more recent versions of Julia, e.g., 1.4 and up.","category":"page"},{"location":"man/binning.html#Binning","page":"Binning methods","title":"Binning","text":"","category":"section"},{"location":"man/binning.html","page":"Binning methods","title":"Binning methods","text":"Pre-specified cuts\nQuantile cuts\nEqual width cuts (histogram cuts)\nEqual density cuts\nEqual value cuts (pareto cuts)","category":"page"}]
}
